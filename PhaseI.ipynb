{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ4Xt1uf1LT7"
   },
   "source": [
    "# Final Project Phase 1 Summary\n",
    "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 1 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
    "\n",
    "Note: To edit a Markdown cell, double-click on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8OjIe2z1LT8"
   },
   "source": [
    "# About Your Topic\n",
    "<br>\n",
    "<b>Q1) What topic have you chosen for your final project?</b> <br>\n",
    "The impact of COVID-19 on the U.S. economy throughout 2020.<br><br>\n",
    "<b>Q2) Why did you choose this specific topic and what are you looking to learn from the analysis? </b> <br>\n",
    "COVID-19 continues to impact the lives of everyone not just in the United States, but worldwide. Today, the country remains divided on whether or not is it advisible to fully reopen the economy inspite of the virus, or to remain in quarantine. My analysis hopes to provide more insight into answering this question.<br>   <br>\n",
    "<b>Q3) Explain some of the concrete insights you expect to gather from your data and/or hypothesis you expect to answer. </b> <br>\n",
    "By comparing the state and national GDP and unemployment rates before and after the arrival of COVID-19 the United States, along with taking into account the trends in number of positive cases, deaths, hospitalizations, and administered tests over the last 7 months, I expect to gain more concrete evidence on whether the economic impact on the U.S. economy outweighs the threat of the virus. Or on other the hand, whether the virus still remains a significant enough threat for the government to take further action to lockdown the country. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJkKO79R1LT9"
   },
   "source": [
    "# About Your Data\n",
    "<br>\n",
    "<b>Q4) Which dataset(s) and websites have you chosen to utilize for your analysis? Include the relevant filenames and links to your datasets. (Please follow the .ipynb hyperlink formatting for links, shown below.) </b> <br>\n",
    "1. https://data.cdc.gov/NCHS/Excess-Deaths-Associated-with-COVID-19/xkkf-xrst ,Filename:Excess_Deaths_Associated_with_COVID-19.csv, 2. https://www.bea.gov/data/gdp/gdp-state ,Filename:qgdpstate1020_0.xlsx, 3. https://covidtracking.com/data/national ,API:https://api.covidtracking.com/v1/us/daily.json ,  4. https://www.ncsl.org/research/labor-and-employment/state-unemployment-update.aspx .<br><br>\n",
    "<b>Q5) For the static file in your dataset collection, state the dimensions of the files in terms of row x columns and the file size (mb, gb, etc.) below. For example, 50,000 rows by 20 columns and 5.4 mb. If your file is a .json file, state the file size (mb, gb, etc.)</b><br>\n",
    "For the file downloaded from https://data.cdc.gov/NCHS/Excess-Deaths-Associated-with-COVID-19/xkkf-xrst , the dimensions of the file are 31,753 rows by 17 columns and 4.6 mb. For the file downloaded from  https://www.bea.gov/data/gdp/gdp-state , the dimensions of the file are 65 rows by 9 columns and 62 kb.<br><br>\n",
    "<b>Q6) Explain why you chose these specific datasets and how they will be used in your analysis. </b><br>\n",
    "In order to understand the full extent of COVID-19 on the American population, I wanted to look at the number of deathes, postive cases, hospitalizations, and tests on both a national and statewide level. In addition, it was important to also look at deathes that were indirectly related to COVID-19, even if the virus was not the primary cause of death in order to get a more accurate death toll. Since GDP and unemployment rate are two of the most important economic indictators, I selected datasets that looked at both statistics on a national and statewide level over a period of many years to look for trends directly related to the timing of the introduction of the virus.  <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K0cxaVA1LT-"
   },
   "source": [
    "## About Your Analysis\n",
    "<br>\n",
    "<b>Q7) Provide a list of steps of how you plan on performing your analysis and the way you will gather/present your findings. (Non-technical, high-level overview) </b><br>\n",
    "I will compare the lifespan of the virus in the U.S. with GDP and unemployment rates over time at both the state and national level in order to gauge the effect of COVID-19 on the economy. I'm currently searching for more datasets that provide historical data for GDP and unemployment rates to gain better perspective on how the current U.S. economy compares to other periods of economic hardship over the past 50-100 years. Furthermore, I plan to look at the total death toll by adding the estimated number excess deaths indirectly to related COVID-19 to the recorded number of deaths on a state and national level. In addition, I will analyze the rate of all COVID stats against time to look for spikes in infections and the overall trends in the number of cases. Through this process, I hope to be able to convey which areas of the U.S. need to continue to enforce lockdowns and which regions can afford to safetly reopen. <br><br>\n",
    "<b>Q8) Explain how you intend on collecting, cleaning, and analyzing the data you gather as well as the manner in which you plan on presenting your insights. (More detailed, include modules, techniques, etc.). </b><br>\n",
    "The main modules that I plan to work with are pandas,numpy,requests,re,json,bs4 and pprint. Since all my data is in the format of rows and columns, I plan to convert each set to a pandas dataframe and clean that data from there. The pandas module will also be used to convert all elements to floats, remove unnecessary punctation, and account for empty cells or NaN values. The numpy module will also be used for removing or replacing NaN values. The requests module will be used for both of my datasets that are scraped from the web, one using an API, and the other with BeautifulSoup from bs4. The re module will be useful for scraping data from the HTML source code of my 4th dataset, as there may possibly be multiple layers of subtags that need to be specifically found. I plan to present my data using line graphs and pandas dataframes that show the rate of all COVID-19 stats side by side with GDP and unemployment rates at the national and state level over time, to establish a correlation between the timing of the virus and the state of economy. <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSiMishC1LT_"
   },
   "source": [
    "## About You\n",
    "<br>\n",
    "\n",
    "<b>Q9) List the name(s) of those working on this topic/project.</b><br>\n",
    "William Kim <br><br>\n",
    "<b> Please Initial Below that you acknowledge this statement: <br>\n",
    "I affirm that all of the work in this project will be done my me/my team and is not duplicated from any other source. In addition, any references that I use or code that I choose to model after will be appropriately credited and referenced in my project. </b><br>\n",
    "WK <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fMkCDZe1LUA"
   },
   "source": [
    "## Your Questions (Optional)\n",
    "<br>\n",
    "Please add any clarifying questions that you would like answered below. Follow the same formatting as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjB_SbWY1LUB"
   },
   "source": [
    "## Jupyter Notebook Quick Tips\n",
    "Here are some quick formatting tips to get you started with Jupyter Notebooks. This is by no means exhaustive, and there are plenty of articles to highlight other things that can be done. We recommend using HTML syntax for Markdown but there is also Markdown syntax that is more streamlined and might be preferable. \n",
    "<a href = \"https://towardsdatascience.com/markdown-cells-jupyter-notebook-d3bea8416671\">Here's an article</a> that goes into more detail. (Double-click on cell to see syntax)\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2\n",
    "### Heading 3\n",
    "#### Heading 4\n",
    "<br>\n",
    "<b>BoldText</b> or <i>ItalicText</i>\n",
    "<br> <br>\n",
    "Math Formulas: $x^2 + y^2 = 1$\n",
    "<br> <br>\n",
    "Line Breaks are done using br enclosed in < >.\n",
    "<br><br>\n",
    "Hyperlinks are done with: <a> https://www.google.com </a> or \n",
    "<a href=\"http://www.google.com\">Google</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9oVjpRDswQ"
   },
   "source": [
    "# Data Collection and Cleaning\n",
    "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
    "\n",
    "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj"
   },
   "source": [
    "## Downloaded Dataset Requirement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "0p5xxmqzFGrO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "def data_parser1(filename):\n",
    "    df=pd.read_csv(filename)\n",
    "    \n",
    "    del df[\"Suppress\"]\n",
    "    del df[\"Note\"]\n",
    "    del df[\"Percent Excess Lower Estimate\"]\n",
    "    del df[\"Percent Excess Higher Estimate\"]\n",
    "    del df[\"Type\"]\n",
    "    del df[\"Outcome\"]\n",
    "    \n",
    "    df.replace(\"\",\"N/A\") #replaces empty cells with \"N/A\"\n",
    "    df.replace(np.nan,\"N/A\") #replaces NaN values with \"N/A\"\n",
    "    df.replace(',','', regex=True, inplace=True) #removes all commas in order to convert to float\n",
    "    df['Observed Number'] = df['Observed Number'].astype(float)\n",
    "    df['Upper Bound Threshold'] = df['Upper Bound Threshold'].astype(float)\n",
    "    df['Average Expected Count'] = df['Average Expected Count'].astype(float)\n",
    "    df['Excess Lower Estimate'] = df['Excess Lower Estimate'].astype(float)\n",
    "    df['Excess Higher Estimate'] = df['Excess Higher Estimate'].astype(float)\n",
    "    df['Total Excess Lower Estimate in 2020'] = df['Total Excess Lower Estimate in 2020'].astype(float)\n",
    "    df['Total Excess Higher Estimate in 2020'] = df['Total Excess Higher Estimate in 2020'].astype(float)\n",
    "    \n",
    "    AllCauses=df[:10584] #splits data by ALl Causes of Death\n",
    "    ExceptCOVID=df[10584:21168] #splits data by All Causes of Death except COVID\n",
    "    \n",
    "    \n",
    "    writer=pd.ExcelWriter(r'C:\\Users\\wkimm\\OneDrive\\Documents\\GT Fall 2020\\CS 2316\\Final Project\\ExcessDeaths.xlsx')\n",
    "    AllCauses.to_excel(writer,sheet_name=\"AllCauses\",index=False) \n",
    "    ExceptCOVID.to_excel(writer,sheet_name=\"ExceptCOVID\",index=False) \n",
    "    writer.save()\n",
    "    \n",
    "    pass\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "pprint(data_parser1(\"Excess_Deaths_Associated_with_COVID-19.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "def data_parser2(filename):\n",
    "    df=pd.read_excel(filename,sheet_name=\"Table 1\")\n",
    "    df.drop(df.index[[0,1,2,63]], inplace=True)\n",
    "    df.replace('.......','N/A', inplace=True)\n",
    "    \n",
    "    df.rename(columns={'Table 1. Percent Change in Real Gross Domestic Product (GDP) by State and Region, 2019:Q1-2020:Q2':'Region/State','Unnamed: 1':'2019','Unnamed: 2':'2019Q1','Unnamed: 3':'2019Q2','Unnamed: 4':'2019Q3','Unnamed: 5':'2019Q4','Unnamed: 6':'2020Q1','Unnamed: 7':'2020Q2','Unnamed: 8':'2020Q2 Rank'}, inplace=True )\n",
    "    \n",
    "    del df[\"2019\"]\n",
    "    \n",
    "    df['2019Q1'] = df['2019Q1'].astype(float)\n",
    "    df['2019Q2'] = df['2019Q2'].astype(float)\n",
    "    df['2019Q3'] = df['2019Q3'].astype(float)\n",
    "    df['2019Q4'] = df['2019Q4'].astype(float)\n",
    "    df['2020Q1'] = df['2020Q1'].astype(float)\n",
    "    df['2020Q2'] = df['2020Q2'].astype(float)\n",
    "    \n",
    "    writer=pd.ExcelWriter(r'C:\\Users\\wkimm\\OneDrive\\Documents\\GT Fall 2020\\CS 2316\\Final Project\\GDP.xlsx')\n",
    "    df.to_excel(writer,index=False) \n",
    "    writer.save()\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "pprint((data_parser2(\"qgdpstate1020_0.xlsx\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection Requirement \\#1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "vXwpJObDFiWM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import json,requests,re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "def web_parser1(url):\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    stats=[] #column headers\n",
    "    for stat in data[0].keys():\n",
    "        stats.append(stat)\n",
    "        \n",
    "    for index,date in enumerate(data):\n",
    "        if date[\"date\"]==20200227:\n",
    "            del data[index+1:] #dels all dates before massive increase(outliers) in test and postive cases\n",
    "        \n",
    "    for date in data:\n",
    "        for stat in stats:\n",
    "            if stat==\"date\":\n",
    "                date.pop(stat) #removes\"date\" column\n",
    "            if stat==\"posNeg\":\n",
    "                date.pop(stat) #removes \"posNeg\" column \n",
    "            if stat==\"hash\":\n",
    "                date.pop(stat) #removes \"hash\" column\n",
    "            if stat==\"hospitalized\": \n",
    "                date.pop(stat) #removes \"hospitalized\" column\n",
    "            if stat==\"total\":\n",
    "                date.pop(stat) #removes \"total\" column\n",
    "            if stat==\"lastModified\": #removes\"lastModified\" column\n",
    "                date.pop(stat)\n",
    "            if stat==\"states\":\n",
    "                date[\"states/territories\"]=date.pop(stat) #clarifies that stat includes US territories\n",
    "            if stat==\"dateChecked\":\n",
    "                m=re.search(r\"(.*?)T00:00:00Z\",date[stat])\n",
    "                if m:\n",
    "                    f=m.group(1)\n",
    "                    date[stat]=f #gets rid of the hours:mins:seconds of the date\n",
    "            if stat==\"death\":\n",
    "                date[\"totaldeaths\"]=date.pop(stat) #each one renames the column header\n",
    "            if stat==\"positive\":\n",
    "                date[\"totalpositivecases\"]=date.pop(stat)\n",
    "            if stat==\"negative\":\n",
    "                date[\"totalnegativecases\"]=date.pop(stat)\n",
    "            if stat==\"hospitalizedCumulative\":\n",
    "                date[\"totalhospitalized\"]=date.pop(stat)\n",
    "            if stat==\"inIcuCumulative\":\n",
    "                date[\"totalinIcu\"]=date.pop(stat)\n",
    "            if stat==\"onVentilatorCumulative\":\n",
    "                date[\"totalonVentilator\"]=date.pop(stat)\n",
    "            if stat==\"recovered\":\n",
    "                date[\"totalrecovered\"]=date.pop(stat)\n",
    "                \n",
    "    for date in data:\n",
    "        for key,val in date.items():\n",
    "            if val==None:\n",
    "                date[key]=0 #replaces all None values with 0\n",
    "                \n",
    "    listoflistvalues=[] #rows of the df\n",
    "    for date in data:\n",
    "        dictvalist=[]\n",
    "        for key,value in date.items():\n",
    "            dictvalist.append(value)\n",
    "        listoflistvalues.append(dictvalist)\n",
    "        \n",
    "    df=pd.DataFrame(listoflistvalues,columns=data[0].keys()) #creates df\n",
    "    df.sort_index(axis=1,inplace=True) #sorts df by column headers\n",
    "    df.to_excel(r'C:\\Users\\wkimm\\OneDrive\\Documents\\GT Fall 2020\\CS 2316\\Final Project\\NationalStats.xlsx',index=False,header=True) #writes df to excel file\n",
    "    \n",
    "    pass\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "pprint((web_parser1(\"https://api.covidtracking.com/v1/us/daily.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection Requirement \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "HAkUOqMgXQJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests,re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "def web_parser2(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    tablehead=soup.find(\"thead\")\n",
    "    headertag=tablehead.find_all(\"th\")\n",
    "    headers=[header.text for header in headertag] #list of headers\n",
    "    del headers[10:]\n",
    "    \n",
    "    tablebody=soup.find(\"tbody\")\n",
    "    statetags=tablebody.find_all(\"tr\")\n",
    "    statesubtag=[state for state in statetags]\n",
    "    statestatlist=[]\n",
    "    for state in statesubtag:\n",
    "        m=re.findall(r\">(.*?)</td>\",str(state))\n",
    "        statestatlist.append(m) #list of lists of states and UR per month\n",
    "        \n",
    "    p=re.search(r\"<p>(.*?)</p>\",str(statesubtag[1])) \n",
    "    alaskafeb=p.group(1)\n",
    "    statestatlist[1].insert(2,alaskafeb) #handles Alaska Feb rate subtag exception\n",
    "        \n",
    "    for state in statestatlist:\n",
    "        del state[10:] #handles Oct-Dec\n",
    "        for index,stat in enumerate(state):\n",
    "            if stat=='\\xa0':\n",
    "                state[index]=\"N/A\" #converts all empty spaces to \"N/A\"\n",
    "            if stat.isdigit():\n",
    "                newstat=float(stat)\n",
    "                strfloat=str(newstat)\n",
    "                state[index]=strfloat #converts all non floats to floats and then back to strings\n",
    "            if stat[0].isdigit():\n",
    "                floatstat=float(stat)\n",
    "                state[index]=floatstat #converts all data to floats\n",
    "        \n",
    "    df=pd.DataFrame(statestatlist,columns=headers) #creates df\n",
    "    df.to_excel(r'C:\\Users\\wkimm\\OneDrive\\Documents\\GT Fall 2020\\CS 2316\\Final Project\\StateUR.xlsx',index=False,header=True) #writes df to excel file\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "pprint(web_parser2(\"https://www.ncsl.org/research/labor-and-employment/state-unemployment-update.aspx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "#Inconsistencies\n",
    "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
    "\n",
    "1. In my first downloaded dataset, there were multiple cases of empty cells. I handled this by replacing the empty cells with \"N/A\". The second downloaded set had empty cells in the form of \".......\" which I also handled by replacing these cells with \"N/A. The dataset scraped by an API had multiple None values which I replaced with 0. The HTML source code had empty cells in form of \"\\xa0\" which I replaced again with \"N/A\".\n",
    "\n",
    "2. In all datasets except for the third scraped with an API, all the elements were either strings, integers or a combination of both instead of all float values. I handled this with a combination of .isdigit() and .astype(float) to homogenize each afflicted column. For columns containing commas, I first removed the commas using .replace() and then followed the previous steps.\n",
    "\n",
    "3. For the dataset scraped using an API, there was large outlier on 2020-02-27. From 2020-02-26 to 2020-02-27, the total number of tests increased from 16 to 6462. In addtion on 2020-02-26, the number of deaths when up by 2 from 0 even though there were no positive tests or many tests at all up until this date. To account for possible inaccuracies before 2020-02-27, I removed all days prior to 2020-02-27.\n",
    "\n",
    "4. For HTML source code, one cell, the unemployment rate for ALaska in February, had a different set of subtags than the rest of the table. I handled this by checking for a different pattern using re.search() and .group(), and then appended the value at the proper index for list of stats for Alaska.\n",
    "\n",
    "5. For my second downloaded dataset, the original formatting of the headers of the xlsx file caused the each header of the loaded dataframe to be unamed. I handled this by reassigning each header to a new name and deleted the beginning rows that caused the formatting issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "## Data Sources\n",
    "Include sources (as links) to your datasets.\n",
    "*   Downloaded Dataset Source:1. https://data.cdc.gov/NCHS/Excess-Deaths-Associated-with-COVID-19/xkkf-xrst 2. https://www.bea.gov/data/gdp/gdp-state\n",
    "*   Web Collection #1 Source: https://covidtracking.com/data/national ,API:https://api.covidtracking.com/v1/us/daily.json\n",
    "*   Web Collection #2 Source: https://www.ncsl.org/research/labor-and-employment/state-unemployment-update.aspx\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
